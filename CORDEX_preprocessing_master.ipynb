{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c904443-76aa-4a61-98b3-9cf2f2f6a2b0",
   "metadata": {},
   "source": [
    "# Steps to Preprocess the CORDEX-Australasia Ensemble for Input into Climpact\n",
    "The code below includes the relevant preprocessing steps taken to prepare the daily tasmin, tasmax, and precip CORDEX-Australasia data for input into Climpact. <br>\n",
    "We use 'preprocessing' here to describe steps taken to process the CORDEX data ahead of using the Climpact software to calculate the indices. <br>\n",
    "Note, the following steps are taken to prepare the daily files:\n",
    "- Merge the daily files into one file (stored as 1 file per year)\n",
    "- Interpolate the files to a consistent 0.5 X 0.5 deg, rectilinear grid (noted as AUS-44i in the file naming)\n",
    "- Create a new catalogue with the necessary metadata to use with the cordex-climpact software (see below)\n",
    "- Additional troubleshooting where needed\n",
    "- Sorting indices into directories based on the experiment and indice name\n",
    "\n",
    "Using Climpact, we then calculate the full set of Climpact climate indices at a common 0.5 x 0.5 degree spatial resolution for the historical, RCP4.5, and RCP8.5 historical simulations. More information about climpact can be found at: https://climpact-sci.org/. A 'full set' of climate indices describes the 51 standardized indices calculated through Climpact. This is not exhaustive of all possible Climate Indices in existence. \n",
    "\n",
    "The revised Climpact software used to batch create the CORDEX-Australasia indices can be found at: https://github.com/coecms/cordex-climpact\n",
    "\n",
    "Note, these preprocessing steps have already been done for the daily, historical and RCP8.5 precipitation datasets. <br>\n",
    "Authors: Rachael N. Isphording, Josh Amoils, and (Alex) Ying Lung Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a4a8890-adbc-4e8d-9341-c0ffe099d006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03337f33-5cf0-4ff8-9d22-46ba609cdbf3",
   "metadata": {},
   "source": [
    "## Step 1: Find the files that we want and concatenate daily files into one dataset\n",
    "Note, this has already been done for the historical and RCP8.5 precip, but needs to be done for the RCP4.5 simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1f486b-b233-46ed-aa77-b098daf09c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List regions (i.e. domains) that we are interested in\n",
    "domains = [\"AUS-22/\", \"AUS-44/\", \"AUS-44i/\"]\n",
    "\n",
    "# Define master paths (below are the two master paths on Gadi for the original CORDEX output)\n",
    "master_path = '/g/data/al33/replicas/cordex/output/'\n",
    "#master_path = '/g/data/rr3/publications/CORDEX/output/'\n",
    "\n",
    "output_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step1_concat/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f097b2-48cc-487d-96b0-4191fd4a356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define useful functions\n",
    "\n",
    "# Get dates from the first and last file to add to the new, concatenated (merged) file name\n",
    "def extract_dates_from_filenames(filenames):\n",
    "    start_date = None\n",
    "    end_date = None\n",
    "\n",
    "    for filename in filenames:\n",
    "        date_range = filename.split('_')[-1].split('.')[0]\n",
    "        range_start, range_end = date_range.split('-')\n",
    "\n",
    "        if start_date is None or range_start < start_date:\n",
    "            start_date = range_start\n",
    "        if end_date is None or range_end > end_date:\n",
    "            end_date = range_end\n",
    "\n",
    "    return start_date, end_date\n",
    "\n",
    "# Merge files using CDO\n",
    "def merge_files(input_file, output_file):\n",
    "    \n",
    "    print(\"!START:\\n input files: \" + input_file + \"\\n output file: \" + output_file)\n",
    "    \n",
    "    !cdo mergetime {input_file} {output_file}\n",
    "\n",
    "    return(print(\"!COMPLETED:\\n input files: \" + input_file + \"\\n output file: \" + output_file))\n",
    "\n",
    "# Recursive function to parse directories and process files\n",
    "def parse_directory(directory, domain):\n",
    "    for root, dirs, files in os.walk(os.path.join(directory, domain)):\n",
    "        if files:\n",
    "            # Filter files based on keywords; the '_' after the variable name ensures that we don't grab the bias corrected or filtered datasets\n",
    "            files = [file for file in files if ('r1i1p1' in file and 'day' in file and ('tasmin_' in file or 'tasmax_' in file or 'pr_' in file))]\n",
    "            if not files:\n",
    "                continue\n",
    "            # Extract start and end dates from filenames\n",
    "            start_date, end_date = extract_dates_from_filenames(files)\n",
    "            \n",
    "            # Remove dates from file name\n",
    "            # Find the length of the substring to remove\n",
    "            substring_length = len(\"_19500101-20051230.nc\")\n",
    "\n",
    "            # Remove the substring using string splicing\n",
    "            new_filename = files[0][:-substring_length] + files[0][:0]\n",
    "            \n",
    "            # Generate output file path\n",
    "            input_file = os.path.join(root, f'{new_filename}_*.nc')\n",
    "            output_file = os.path.join(output_path, domain, f'{new_filename}_{start_date}-{end_date}.nc')\n",
    "            \n",
    "            # Check if file already exists\n",
    "            if os.path.exists(output_file):\n",
    "                \n",
    "                print(\"File already exists: \" + output_file)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # Merge files using CDO command\n",
    "                merge_files(input_file, output_file)\n",
    "\n",
    "# Call the function to start parsing the master directory\n",
    "# DOMAINS in all-caps to prevent accidental running\n",
    "for domain in DOMAINS:\n",
    "    parse_directory(master_path, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aedf51-ac4e-4e33-a73b-65b452ac73b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Interpolate and remap the AUS-22i datasets to AUS-44i (0.5 X 0.5 deg)\n",
    "Note, bilinear interpolation will be used for temperature data and first order conservative regridding will be used for precipitation data <br>\n",
    "The 'my_projection_AUS22.txt' file was created prior to running the section below using the CDO command: <br>\n",
    "cdo griddes tasmin_AUS-22_NCC-NorESM1-M_rcp85_r1i1p1_ICTP-RegCM4-7_v0_day_20060101-20991231.nc > my_projection_AUS22.txt <br>\n",
    "This step assumes a consistent grid description for files of the same domain. <br>\n",
    "Also note, the AUS-44 files had previously been interpolated to the AUS-44i grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dabe27d9-501d-4288-ac4b-0a9361e7c41b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List regions (i.e. domains) that we are interested in and time periods for sorting\n",
    "domains = [\"AUS-22\"]\n",
    "time_periods = [\"historical\", \"rcp45\", \"rcp85\"]\n",
    "\n",
    "# Define input and output directories\n",
    "input_master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step1_concat/'\n",
    "output_master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62860639-2eed-4060-8bac-1b85b7f2903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through directories \n",
    "# DOMAINS changed to all-caps to prevent accidental running of the script\n",
    "for domain in DOMAINS:\n",
    "    \n",
    "    for time_period in time_periods:\n",
    "        \n",
    "        # Iterate through the files in the given directory\n",
    "        for filename in os.listdir(input_master_path + domain + '/' + time_period):\n",
    "            \n",
    "            print(\"Starting: \", filename)\n",
    "            \n",
    "            # Define new filename after regridding and full output path\n",
    "            new_filename = filename.replace(f\"{domain}\", \"AUS-44i\")\n",
    "            new_full_output_path = os.path.join(output_master_path, time_period, new_filename)\n",
    "            \n",
    "            print(\"New output path : \", new_full_output_path)\n",
    "            \n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(new_full_output_path):\n",
    "                print(\"File already exists: \" + new_full_output_path)\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Set the gridtype and store the changed gridtype as 'temp.nc'\n",
    "                !cdo setgrid,{input_master_path + 'my_projection_AUS22.txt'} {input_master_path + domain + '/' + time_period + '/' + filename} {output_master_path + 'temp.nc'}\n",
    "            \n",
    "                # Perform the regridding using cdo remapycon (first order conservative regriddingand save in the folder based on the time period\n",
    "                if filename.startswith('pr_AUS-22'):\n",
    "                \n",
    "                    !cdo remapycon,{output_master_path + 'AUS_44i_time1.nc'} {output_master_path + 'temp.nc'} {new_full_output_path}\n",
    "            \n",
    "                # perform bilinear interpolation for the temperature files\n",
    "                elif filename.startswith('tasm'):\n",
    "                \n",
    "                    !cdo remapbil,{output_master_path + 'AUS_44i_time1.nc'} {output_master_path + 'temp.nc'} {new_full_output_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9a9d0-8dec-42a8-a791-df7589363d51",
   "metadata": {},
   "source": [
    "## Step 3: Create a new intake catalogue with necessary metadata for the interpolated files\n",
    "This intake catalogue file needs to be updated in the preprocess.py script of the cordex-climpact software. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "763aca62-12e5-454a-8c31-c44545369bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original catalogue\n",
    "old_catalogue = pd.read_csv('/g/data/w40/ri9247/josh/catalogue_AUS44i.csv')\n",
    "\n",
    "# Option to print original catalogue for sanity check\n",
    "old_catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea516297-f4ba-49e9-b74d-a4797f0cd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new catalogue with the same columns as the original catalogue\n",
    "new_catalogue = pd.DataFrame(columns = old_catalogue.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfe221b5-fdbb-4bcb-9c86-e9f07f782ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define constants for new catalogue\n",
    "project = 'cordex'\n",
    "product = 'output'\n",
    "\n",
    "# The version is retained in the file's metadata and gets dropped in Scott's code. For efficiency, we'll force a version, choosing the birthday shared by me and Baby Spice. It is also National Hug Day. \n",
    "version = 'v19920121'\n",
    "\n",
    "# Define list with experiments\n",
    "experiments = [\n",
    "    'historical',\n",
    "    'rcp85',\n",
    "    'rcp45'\n",
    "]\n",
    "\n",
    "# Define master path of input datasets\n",
    "master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb038c-cb94-43b4-9f1c-c721a16c66d5",
   "metadata": {},
   "source": [
    "**NOTES:** <br>\n",
    "Loop through the directory of input data and fill in the rest of the columns in the catalogue. <br>\n",
    "Note that the files follow the following naming convention: <br>\n",
    "{variable} _ {domain} _ {driving_model} _ {experiment} _ {ensemble} _ {institue}-{rcm_name} _ {rcm_version} _ {time_frequency} _ {date_range}.nc <br>\n",
    "{variable} _ {domain} _ {driving_model} _ {experiment} _ {ensemble} _ {rcm_name_id} _ {rcm_version} _ {time_frequency} _ {date_range}.nc <br>\n",
    "An example: <br>\n",
    "tasmin _ AUS-44i _ NOAA-GFDL-GFDL-ESM2M _ rcp85 _ r1i1p1 _ CSIRO-CCAM-2008 _ v2 _ day _ 20060101-20991231.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af7b5615-61c4-44d2-ad21-586cb0d94abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTS in all caps to prevent accidental running\n",
    "# Loop through the time period (aka experiments) directories in our pre-processed files\n",
    "for experiment in EXPERIMENTS:\n",
    "    directory = master_path + experiment\n",
    "    \n",
    "    # Get the individual files\n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        # Extract the different attributes from the filename\n",
    "        attributes = filename.split('_')\n",
    "        \n",
    "        # Define each attribute in the list (see above file naming structure\n",
    "        variable = attributes[0]\n",
    "        domain = attributes[1]\n",
    "        driving_model = attributes[2]\n",
    "        # experiment is defined via the looping\n",
    "        ensemble = attributes[4]\n",
    "        rcm_name_id = attributes[5]\n",
    "        \n",
    "        # A bit of hard-coding because the institute name for 'CLMcom-HZG' has a hyphen in it\n",
    "        if 'CLMcom-HZG' in rcm_name_id:\n",
    "            institute = 'CLMcom-HZG'\n",
    "            rcm_name = 'CCLM5-0-15'\n",
    "              \n",
    "        else:\n",
    "            # split the rcm_name_id at the first hyphen to get the institute and the rcm_name\n",
    "            rcm_attributes = rcm_name_id.split('-', 1)\n",
    "            institute = rcm_attributes[0].strip()\n",
    "            rcm_name = rcm_attributes[1].strip()\n",
    "            \n",
    "        rcm_version = attributes[6]\n",
    "        time_frequency = attributes[7]\n",
    "        date_range = attributes[8]\n",
    "        \n",
    "        # Define the full path\n",
    "        path = master_path + experiment + '/' + filename\n",
    "        \n",
    "        # Make a new dataframe with the file entry\n",
    "        catalogue_entry = pd.DataFrame({'rcm_name': rcm_name,\n",
    "                                        'driving_model': driving_model,\n",
    "                                        'experiment': experiment,\n",
    "                                        'path': path,\n",
    "                                        'project': project,\n",
    "                                        'product': product,\n",
    "                                        'domain': domain,\n",
    "                                        'institute': institute,\n",
    "                                        'ensemble': ensemble,\n",
    "                                        'rcm_name_id': rcm_name_id,\n",
    "                                        'rcm_version': rcm_version,\n",
    "                                        'time_frequency': time_frequency,\n",
    "                                        'variable': variable,\n",
    "                                        'version': version,\n",
    "                                        'date_range': date_range\n",
    "                                       }, index=[0])\n",
    "        \n",
    "        # Add everything to the catalogue\n",
    "        new_catalogue = pd.concat([new_catalogue, catalogue_entry], ignore_index = True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33795df2-bdbd-43bd-a465-a0f9325ef56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to print new_catalogue to check\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "new_catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df777a70-50db-48db-b4c0-3300e31f2def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new catalogue to a csv file\n",
    "new_catalogue.to_csv('/g/data/w40/ri9247/josh/catalogue.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56560488-0aa8-4939-9897-993f56683406",
   "metadata": {},
   "source": [
    "## Step 4 - Troubleshooting\n",
    "Here, we troubleshoot a few simulations with unique pre-processing issues including:\n",
    "- subsetting historical and RCP85 WRF pr files to match the latitude and longitude of the tasmin/tasmax\n",
    "- correcting CCAM-2008 calendar mismatch\n",
    "- NOAA-GFDL-GFDL-ESM2M CCAM-2008 troubleshooting: fill February 2003 with missing data values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38e00b-1fce-4a8b-a60c-e4e02e050b4a",
   "metadata": {},
   "source": [
    "### Subsetting WRF pr files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e624943a-93d4-4f07-b580-1bee31a3c7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define list with experiments\n",
    "experiments = [\n",
    "    'historical',\n",
    "    'rcp85'\n",
    "]\n",
    "\n",
    "# Define master path of input datasets\n",
    "input_master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/tmp_WRF_pr_files/'\n",
    "output_master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dafd54-d9f8-4c75-9cba-c9c36b67577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTS in all caps to prevent accidental running\n",
    "# Loop through the time period (aka experiments) directories in our pre-processed files\n",
    "for experiment in EXPERIMENTS:\n",
    "        \n",
    "    # Iterate through the files in the given directory\n",
    "    for filename in os.listdir(input_master_path + experiment):\n",
    "            \n",
    "        print(\"Starting: \", filename)\n",
    "        \n",
    "        # Use CDO to subset lat and lon boundaries\n",
    "        !cdo -sellonlatbox,89.75,205.75,9.75,-51.75 {input_master_path + experiment + '/' + filename} {output_master_path + experiment + '/' + filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89049e21-17d3-465e-8481-74f6093c1e65",
   "metadata": {},
   "source": [
    "### Correcting the calendar for CCAM-2008 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de8d8fb-9951-4656-b62e-e25ef557b07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define list with experiments\n",
    "experiments = [\n",
    "    'historical',\n",
    "    'rcp45',\n",
    "    'rcp85'\n",
    "]\n",
    "\n",
    "# Define master path of input datasets\n",
    "input_master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/tmp_CCAM_calendar/'\n",
    "output_master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9cbe6-eb42-4021-b684-d6c99b09a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTS in all caps to prevent accidental running\n",
    "# Loop through the time period (aka experiments) directories in our pre-processed files\n",
    "for experiment in EXPERIMENTS:\n",
    "        \n",
    "    # Iterate through the files in the given directory\n",
    "    for filename in os.listdir(input_master_path + experiment):\n",
    "        \n",
    "        print(\"Starting: \", filename)\n",
    "        \n",
    "        # Use CDO to update the calendar\n",
    "        # Make the unit of equal time\n",
    "        !cdo -setreftime,1900-01-01,00:00:00,1day {input_master_path + experiment + '/' + filename} {'temp_cal_step1.nc'}\n",
    "        \n",
    "        # Make all files share the 365-day calendar\n",
    "        !cdo -setcalendar,365_day {'temp_cal_step1.nc'} {output_master_path + experiment + '/' + filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e415b2-5b57-4470-beeb-937429f3a898",
   "metadata": {},
   "source": [
    "### NOAA-GFDL-GFDL-ESM2M CCAM-2008 fill February 2003 with missing data values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59a261bc-4171-4c88-89e1-0de09ab9d113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define path to data input and output\n",
    "master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/tmp_CCAM_missing_Feb/'\n",
    "\n",
    "# Files we need to fill with missing values for February 2003\n",
    "input_pr = 'pr_AUS-44i_NOAA-GFDL-GFDL-ESM2M_historical_r1i1p1_CSIRO-CCAM-2008_v1_day_19600101-20051231.nc'\n",
    "input_tasmin = 'tasmin_AUS-44i_NOAA-GFDL-GFDL-ESM2M_historical_r1i1p1_CSIRO-CCAM-2008_v1_day_19600101-20051231.nc'\n",
    "input_tasmax = 'tasmax_AUS-44i_NOAA-GFDL-GFDL-ESM2M_historical_r1i1p1_CSIRO-CCAM-2008_v1_day_19600101-20051231.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2429dd15-ef88-4c4a-8782-ef0a879914cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing netCDF data\n",
    "existing_pr = xr.open_dataset(master_path + input_pr)\n",
    "existing_tasmin = xr.open_dataset(master_path + input_tasmin)\n",
    "existing_tasmax = xr.open_dataset(master_path + input_tasmax)\n",
    "\n",
    "# Define the time range we need to fill\n",
    "start_date = np.datetime64('2003-02-01')\n",
    "end_date = np.datetime64('2003-02-28')\n",
    "\n",
    "# Create a new time axis with the needed time range\n",
    "new_time = np.arange(start_date, end_date + np.timedelta64(1, 'D'), np.timedelta64(1, 'D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cedd585-c8c2-4e43-8dbd-ada6ed7ea9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define missing value based on value in file's metadata\n",
    "missing_value_fill = 1.e+20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4030485-4782-4301-adb8-57d11e7083ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create new datasets with the copied metadata\n",
    "new_pr = xr.Dataset(attrs=existing_pr.attrs)\n",
    "new_pr['time'] = new_time\n",
    "\n",
    "new_tasmin = xr.Dataset(attrs=existing_tasmin.attrs)\n",
    "new_tasmin['time'] = new_time\n",
    "\n",
    "new_tasmax = xr.Dataset(attrs=existing_tasmax.attrs)\n",
    "new_tasmax['time'] = new_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ca8bad-fbe8-4560-8ff2-091f1a00545e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new DataArray with missing values\n",
    "# Precipitation\n",
    "missing_data_pr = np.full((len(new_time), len(existing_pr['lat']), len(existing_pr['lon'])), missing_value_fill)\n",
    "new_pr['pr'] = xr.DataArray(missing_data_pr,\n",
    "                                  coords={'time': new_time, 'lat': existing_pr['lat'], 'lon': existing_pr['lon']},\n",
    "                                  dims=['time', 'lat', 'lon'])\n",
    "\n",
    "# Tasmin\n",
    "missing_data_tasmin = np.full((len(new_time), len(existing_tasmin['lat']), len(existing_tasmin['lon'])), missing_value_fill)\n",
    "new_tasmin['tasmin'] = xr.DataArray(missing_data_tasmin,\n",
    "                                  coords={'time': new_time, 'lat': existing_tasmin['lat'], 'lon': existing_tasmin['lon']},\n",
    "                                  dims=['time', 'lat', 'lon'])\n",
    "\n",
    "# Tasmax\n",
    "missing_data_tasmax = np.full((len(new_time), len(existing_tasmax['lat']), len(existing_tasmax['lon'])), missing_value_fill)\n",
    "new_tasmax['tasmax'] = xr.DataArray(missing_data_tasmax,\n",
    "                                  coords={'time': new_time, 'lat': existing_tasmax['lat'], 'lon': existing_tasmax['lon']},\n",
    "                                  dims=['time', 'lat', 'lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3538e81f-a64d-4dd5-8287-593297aedb0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write missing data files to new datasets\n",
    "new_pr.to_netcdf(master_path + 'pr_Feb2003_missing_fill.nc')\n",
    "new_tasmin.to_netcdf(master_path + 'tasmin_Feb2003_missing_fill.nc')\n",
    "new_tasmax.to_netcdf(master_path + 'tasmax_Feb2003_missing_fill.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3623875b-3ab1-43d1-8983-c10e82e2994d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a4351d2-470a-4af3-a6a6-fdf57d209728",
   "metadata": {},
   "source": [
    "### Convert calendar types to a consistent type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c48931d4-d723-4d05-bdd0-aae4da2c1b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_path = '/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/tmp_CCAM_missing_Feb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca096e1c-b82a-4ddd-b7c5-85e24a099b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the files in the given directory\n",
    "for filename in os.listdir(master_path):\n",
    "        \n",
    "    print(\"Starting: \", filename)\n",
    "        \n",
    "    # Use CDO to update the calendar\n",
    "    # Make the unit of equal time\n",
    "    !cdo -setreftime,1900-01-01,00:00:00,1day {master_path + filename} {master_path + 'temp1_' + filename}\n",
    "        \n",
    "    # Make all files share the 365-day calendar\n",
    "    !cdo -setcalendar,365_day {master_path + 'temp1_' + filename} {master_path + 'step2_' + filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d94819-8587-49ac-ade5-6ea2062d06b1",
   "metadata": {},
   "source": [
    "**NOTES:** <br>\n",
    "Used cdo mergetime in the terminal to merge the step2_{variable}_*.nc files <br>\n",
    "Useful scripts below to check that filling worked correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a328b-b99e-4a58-8507-c1395d868635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correct number of days in each year\n",
    "f_pr = xr.open_dataset('/scratch/w40/ri9247/tmp/cordex_preproc/step2_regrid_AUS-44i/tmp_CCAM_missing_Feb/filled_tasmin_AUS-44i_NOAA-GFDL-GFDL-ESM2M_historical_r1i1p1_CSIRO-CCAM-2008_v1_day_19600101-20051231.nc')\n",
    "for y in range(1960,2006):\n",
    "    ts_pr = f_pr.time.sel(time=slice(str(y)+'-01',str(y)+'-12'))\n",
    "    print(str(y)+': '+str(len(ts_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d6c52-7aff-461e-83d7-95d2c8d44bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the months have the correct number of days\n",
    "for m in range(1,13):\n",
    "    month = str(m).zfill(2)\n",
    "    ts_pr = f_pr.time.sel(time=slice('2003-'+month,'2003-'+month))\n",
    "    print(month+': '+str(len(ts_pr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0244142-3bd7-409b-96a9-216abad26c51",
   "metadata": {},
   "source": [
    "## Copy indices into folders based on the experiment and index name\n",
    "Note, ahead of doing this, the empty folders were copied from an existing directory using: <br>\n",
    "find . -maxdepth 1 -type d > /scratch/w40/ri9247/tmp/dirs.txt <br>\n",
    "xargs mkdir -p < /scratch/w40/ri9247/tmp/dirs.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6758da-955c-4357-aa93-3c8e74033e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original output directory of Climpact indices\n",
    "root_dir_climpact = '/g/data/w40/yl1269/climpact_indices/cordex_AUS44i_Aug2023_all_indices/'\n",
    "\n",
    "# Master destination directory\n",
    "dest_root_dir = '/scratch/w40/ri9247/tmp/cordex_indices/'\n",
    "\n",
    "# List of filenames created that are not climate indices that we will not sort\n",
    "non_index_climpact_filenames = ['thresholds.done'\n",
    "    , 'thresholds.log'\n",
    "    , 'thresholds.nc'\n",
    "    , 'climpact.done'\n",
    "    , 'climpact.log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8779e504-2894-4d54-b66b-713e8ab8aec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the original climpact output directory (files are sorted based on the simulation and the experiment)\n",
    "for model_dir in os.listdir(root_dir_climpact):\n",
    "    \n",
    "    # Print directory name for tracking progress\n",
    "    print(model_dir)\n",
    "    \n",
    "    # Get the experiment based on the original output directory name\n",
    "    if 'historical' in model_dir:\n",
    "        experiment = 'historical/'\n",
    "        \n",
    "    elif 'rcp45' in model_dir: \n",
    "        experiment = 'rcp45/'\n",
    "        \n",
    "    elif 'rcp85' in model_dir: \n",
    "        experiment = 'rcp85/'\n",
    "    \n",
    "    # For each directory loop through the files and copy the indices files into the appropriate folder\n",
    "    for file in os.listdir(root_dir_climpact + model_dir):\n",
    "        \n",
    "        # If the file is not a climate index file, skip it\n",
    "        if file in non_index_climpact_filenames: continue\n",
    "        \n",
    "        # Get the file info\n",
    "        attributes = file.split(\"_\")\n",
    "        \n",
    "        # Get index\n",
    "        index = attributes[0] + '/'\n",
    "        \n",
    "        # Define the new directory\n",
    "        new_directory = dest_root_dir + experiment + index\n",
    "        \n",
    "        # Copy the file to the new, sorted directory\n",
    "        !cp {root_dir_climpact + model_dir + '/' + file} {new_directory + file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f73e82-5f88-40ee-875b-98bea01d0503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3]",
   "language": "python",
   "name": "conda-env-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
